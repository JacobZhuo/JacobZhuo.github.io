---
title: ' Dual Gradients: IG^{2} - A SOTA Integrated Gradients Variant for XAI'
date: 2023-12-4
permalink: /posts/2023/12/blog-post-1/
tags:
  - eXplainable AI
---

# Dual Gradients: IG^{2} - A SOTA Integrated Gradients Variant for XAI

Integrated Gradients (IG) is a gradient-based approach for explaining deep models. IG methods integrate the gradient along a path for attributing the feature importance. With many desirable axioms, path (attribution) methods achieve great success over the simple gradient techniques. However, although many IG variants have been proposed, there still remains a lot of discussions about the two primary hyperparameters (e.g., path and baselines). In this post, I will introduce IG^{2} (**I**terative **G**radient path **I**ntegrated **G**radients), a IG variant with novel path, named GradPath, and novel baseline, GradCF. IG^{2} achieves superior performance over the SOTA feature attribution technique.

## Understanding Integrated Gradients

Integrated Gradients (IG) is an Explainable AI technique introduced in the paper [``Axiomatic Attribution for Deep Networks&#39;&#39;](https://arxiv.org/abs/1703.01365). IG provides feature attribution for deep neural networks, quantifying the contributions of individual features, such as pixels, to the model output. These results can support the users in reasoning which input elements drive the model predictions.

Gradients is a naive feature attribution, which analogizes the model coefficients for a deep network. Early methods such as Saliency Map, Grad-CAM and Guided Backpropagation are representative based on local gradient. However, the local gradients in the input neighborhood are misleading, which is known as the gradient saturation effect.

![gradient saturation](image/blog_post/1701417373872.png "Image by Krishnaram Kenthapadi ([Explainable AI in Industry](https://www.slideshare.net/KrishnaramKenthapadi/explainable-ai-in-industry-kdd-2019-tutorial) (KDD 2019 Tutorial))"){width="50%"}

**Gradient saturation effect** is caused by the flat and smooth loss landscape of well-trained neural networks. The model prediction is very stable in the local neighborhood around the input instance. This results in the noisy gradients on irrelative features, whereas only the gradients leading to model prediction change are of interest.

IG is a path (attribution) method to address this effect by accumulating all the gradients along the path between the explained instance (i.e., **explicand**) and **baseline** (introduce below). Path methods rooted in Aumann-Shapley game theory, adhere to many describable axioms and IG has recently become a popular technique for explaining deep models.

**Baseline** is one of two primary hyperparameters of path methods (another is the integration path). Baseline introduces the concept of counterfactual explanation, which contrastively explain the models by answering:

> Which features cause the model output prediction A (of explicand) rather than counterfactual prediction B (of baseline)?

From the perspectives of philosophy and psychology, the counterfactuals align with human cognition to explain unexpected events, and have been widely applied in XAI techniques, such as Shapley-value based attribution methods like SHapley Additive exPlanations (SHAP) and DeepLIFT.

### IG variants

Almost all the variant path methods are dedicated into designing better integration path or baseline for improving feature attributions:

* **Paths:** Blur IG integrated the gradients on the gradually blurred image path. Guided IG adaptively chooses the path by selecting features with the smallest partial derivatives. [Split IG](https://arxiv.org/abs/2010.12697v1) restricted the integral to regions with interesting gradients where the model output changes substantially.
* **Baselines:** Sturmfels et al. discussed various baselines’ impacts on the path methods, and expected IG sampled the baselines from the data distribution. Blur IG applied a blurred explicand as the baseline.

Table below summarizes the existing IG-based methods from the aspects of path and baseline, comparing with our proposal IG^{2}.

| Methods          | Path                        | Baseline           |
| ---------------- | --------------------------- | ------------------ |
| IG               | straight line               | zero vector        |
| Expected IG      | straight line               | train data         |
| XRAI             | straight line               | black+white images |
| Blur IG          | blur path                   | blurred image      |
| Split IG         | part of straight line      | zero vector        |
| Guided IG        | projection of straight line | zero vector        |
|                  |                             | maximal distance   |
| Sturmfels et al. | straight line               | noised data        |
|                  |                             | uniform noise      |
| **IG^{2}** | **GradPath**          | **GradCF**   |

<center>Summary of existing path methods from the aspects of path and baseline. </center>

## IG^: Advancing on Previous Success

IG^{2} advances in feature attribution by novel path and baseline. They both show the positive effect on path attribution. From these two essential parts, I will introduce IG^{2} in the following, explaining how and why they work.

### GradPath: Mitigating Saturation Effects

As introduced before, IG is proposed for addressing the saturation effect in local gradient methods. However, though IG contains interesting gradients, the saturation area with noisy gradients are inevitably traversed. Some previous research with improved paths have been proposed for this issue. Let us see how they work.

![1701588009617](image/blog_post/1701588009617.png)
<center>Illustration of paths in Split IG, Guided IG and IG^{2}, taking a Digit 5 instance in MNIST as explicand and Digit 6 instance as baseline.  </center>

**Split IG** avoids the saturation effects by only integrating on the region that model prediction is rapidly decreasing.

**Guided IG** is more sophisticated. From explicand to baseline, it first integrated gradients on the directions with the largest gradient. However, each step of Guided IG only directs to partial components of straight vector. This restricts the path of Guided IG on the projection of straight line.

The previous research work has proven that integrating on the paths where model prediction drastically declines leads to better attribution results. However, these paths were still modified based on straight line between baseline and explicand. It is natural to think that:

> Why not build a path that makes the model prediction fall the fastest?

This is highly similar to the gradient-based adversarial attack (e.g, PGD attack). Given a sample $x$, PGD attack iteratively searches the adversarial example $x'$ by:
$$x' = x+ \epsilon \cdot \nabla f(x)$$
where each perturbation step is built on the gradient direction $\nabla f(x)$.

But only making the model prediction decrease fast is not enough. When we build integration path, we want to search a counterfactual example, used to explain the difference between explicand and the counter class sample. Thus, we built the GradPath on the gradient of representation difference between explicand and counter class reference, $x^r$:
$$ \nabla \|f'(x) - f'(x^r) \|^2_2$$
where $f'$ is the bottleneck layer output of deep neural networks. 

At the endpoint of path search, we obtain the baseline of IG^{2}, GradPath.

> In IG^{2}, we do not directly use the counter class samples as the baseline (which is applied in expected IG). Instead, the baseline (i.e, GradCF) is calculated based on the counter class samples by the equation above. We name this counter class sample as the reference.

![1701588009617](image/blog_post/gradpath.png){width="50%"}

The figure shows how GradPath is built at each step, which is on the counterfactual direction for minimizing the model representation distance to the reference. On one hand, the path completely avoids the saturated area. On the other hand, this path direction emphasizes the features that are most significant for distinguishing the exlicand and counter class reference.

![1701588009617](image/blog_post/gradpath_Doberman.png){width="50%"}

**ImageNet Example:** Next, I will use a Doberman instance from ImageNet to show the effect of GradPath. The figure compares the attribution snapshots of three different paths at the point of explicand ($\alpha=1$). The path attribution snapshot at certain point is calculated by multiplication of explicand's gradient and integration direction. At $\alpha=1$, different paths share the same explicand's gradient and only differs in the integration direction. 
Shown by Doberman example, the straight line path with saturated areas cause undesirable noise in the background. Guided IG and IG^{2} effectively mitigate this saturation effect. Compared to Guided IG, IG^{2} is more aligned with the image subject (Doberman), leading to the attribution that accurately highlights the image region of dog body, which is also more aligned with human intuition.

### GradCF: Contrasting at Semantic Level
